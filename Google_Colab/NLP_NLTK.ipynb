{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pyspellchecker\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial de NLTK para NLP\n",
    "\n",
    "En este corto tutorial veremos algunas de las herramientas y m√©todos que tiene NLTK para el procesamiento de texto. Tambi√©n nos apoyaremos en la librer√≠a [RE](https://docs.python.org/3/library/re.html) (Expresiones Regulares) para la limpieza del texto. Empezamos creando una lista de Tweets sint√©ticos, ya que lo que nos interesa en este Notebook es la parte del procesamiento. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\patricio.fernandez\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patricio.fernandez\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\patricio.fernandez\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer # tokenizador\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords # elimina palabras de poco valor\n",
    "from nltk.stem.wordnet import WordNetLemmatizer # stemming lemming\n",
    "# Instalamos algunas de las APIs y herramientas de NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestras stopwords\n",
    "\n",
    "stopwords_ = set(stopwords.words('english') + list(punctuation)+ [\"rt\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"Hellooooooo!! WHat's wrong with the world today!!!! @user1 #bored #world\",\n",
    "    \"RT I simply looove this outfit www.fakeoutfitpage.com, it's my favorite. I can't stop thinking about it!! @shopingmall #fashion\",\n",
    "    \"The launch of the @SpaceY has been cancelled, I don't want to believe this :( . #notcool #spacey\",\n",
    "    \"RT Hey! Visit www.thisisafakeurl.com to get a FREEEEE coupon on ALL the @fakeshop productos. #Freecoupon\",\n",
    "    \"What a nice album @hotplay has released, I JUST CAN'T STOP LISTENING TO IT. THE BEEEEEEEEEST!! #hotplay #music\"    \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos a trabajar con un Tweet para revisar cada paso, luego creamos una funci√≥n para realizarlo en todo el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" RT I simply looove this outfit www.fakeoutfitpage.com, it's my favorite. I can't stop thinking about it!! @shopingmall #fashion\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos a trabajar con un tweet para ir viendo cada paso\n",
    "tweet = tweets[1]\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" rt i simply looove this outfit www.fakeoutfitpage.com, it's my favorite. i can't stop thinking about it!! @shopingmall #fashion\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empezamos transformando a min√∫sculas\n",
    "tweet = tweet.lower()\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" rt i simply looove this outfit URL it's my favorite. i can't stop thinking about it!! @shopingmall #fashion\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removemos la URL usando expresiones regulares\n",
    "tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet)\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'usuario'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"@usuario\".replace(\"@\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" rt i simply looove this outfit URL it's my favorite. i can't stop thinking about it!! USUARIO #fashion\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removemos el usario expresiones regulares\n",
    "tweet = re.sub('@[^\\s]+', 'USUARIO', tweet)\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" rt i simply looove this outfit URL it's my favorite. i can't stop thinking about it!! USUARIO fashion\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removemos el hashtag\n",
    "tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos la repetici√≥n de caracteres en LOOOOOOOOOOVE\n",
    "tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', 'i', 'simply', 'loove', 'this', 'outfit', 'URL', 'it', \"'s\", 'my', 'favorite', '.', 'i', 'ca', \"n't\", 'stop', 'thinking', 'about', 'it', '!', '!', 'USUARIO', 'fashion']\n"
     ]
    }
   ],
   "source": [
    "# probamos los tokenizadores que cargamos \n",
    "# word tokenizer \n",
    "print(word_tokenize(tweet)) # remove repeated characters (helloooooooo into hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calle = {\"Av\":\"Aveniida\",\"Av.\", \n",
    "        \"Calle\": \"calle\",\"cl\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No me gusta mucho el resultado ya que palabras como it's se dividen, igual can't. Probamos el otro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', 'i', 'simply', 'loove', 'this', 'outfit', 'URL', \"it's\", 'my', 'favorite', '.', 'i', \"can't\", 'stop', 'thinking', 'about', 'it', '!', '!', 'USUARIO', 'fashion']\n"
     ]
    }
   ],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "print(tknzr.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es mejor, y adem√°s est√° dise√±ado para Twitter :). Lo utilizaremos de ahora en adelante "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', 'i', 'simply', 'loove', 'this', 'outfit', 'URL', \"it's\", 'my', 'favorite', '.', 'i', \"can't\", 'stop', 'thinking', 'about', 'it', '!', '!', 'USUARIO', 'fashion']\n"
     ]
    }
   ],
   "source": [
    "# TOkenizamos \n",
    "tweet = tknzr.tokenize(tweet)\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simply', 'loove', 'outfit', 'URL', 'favorite', \"can't\", 'stop', 'thinking', 'USUARIO', 'fashion']\n"
     ]
    }
   ],
   "source": [
    "# Quitamos las stopwords\n",
    "tweet = [word for word in tweet if word not in stopwords_] # nuestras stopwords\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simply', 'loove', 'outfit', 'URL', 'favorite', \"can't\", 'stop', 'think', 'USUARIO', 'fashion']\n"
     ]
    }
   ],
   "source": [
    "def stem_lemm(tweet_list):\n",
    "    \"\"\"\n",
    "    Lemmatiza un tweeet uitlizando el lematizador de NLTK\n",
    "    :param tweet_list: Un tweet tokenizado previamente\n",
    "    :return: Una lista de tokens\n",
    "    \"\"\"\n",
    "    lem = WordNetLemmatizer()\n",
    "    normalized_tweet = []\n",
    "    for word in tweet_list:\n",
    "        normalized_text = lem.lemmatize(word,'v')\n",
    "        normalized_tweet.append(normalized_text)\n",
    "    return normalized_tweet\n",
    "\n",
    "print(stem_lemm(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola que tal\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Estas siguientes lines de c√≥digo hacen algo\n",
    "Algo interesante \n",
    "haha\n",
    "hahah\n",
    "nanan\n",
    "nanan\n",
    "\n",
    "\n",
    "'''\n",
    "#\"\" funcion que hace algo\n",
    "#\n",
    "#\n",
    "s = \"que tal\"\n",
    "print(f\"hola {s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\njsjsjsjsj\\ndldldldldldl\\ndldeoodkd\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_largo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcci√≥n gramatical \n",
    "\n",
    "En muchos casos tambi√©n es posible que necesitemos a√±adir un corrector gramatical para nuestro proyecto. Eso se puede hacer mediante varias librer√≠as, pero una de las m√°s comunes es SpellChecker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "{'love', 'looe', 'loose'}\n",
      "usuario\n",
      "{'usuario'}\n",
      "usl\n",
      "{'curl', 'ural', 'ura', 'uri', 'urn', 'crl', 'usl', 'ucl', 'rl', 'ure', 'urc', 'uhl', 'urd', 'erl', 'hurl', 'purl', 'brl', 'ur', 'ual', 'urr', 'u/l'}\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Encuentra las que tienen errores\n",
    "misspelled = spell.unknown(tweet)\n",
    "\n",
    "for word in misspelled:\n",
    "    # La correci√≥n m√°s probable\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # otros conadidatos\n",
    "    print(spell.candidates(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simply',\n",
       " 'love',\n",
       " 'outfit',\n",
       " 'URL',\n",
       " 'favorite',\n",
       " \"can't\",\n",
       " 'stop',\n",
       " 'thinking',\n",
       " 'USUARIO',\n",
       " 'fashion']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Corregimos el tweet. \n",
    "[spell.correction(word) if word in misspelled else word for word in tweet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo junto \n",
    "\n",
    "Ahora que hemos visto y probado varios m√©todos. Podemos hacer una funci√≥n o una clase con todo lo necesario para limpiar nuestros Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpia_tweets(tweet): \n",
    "    # empezamos transformando a min√∫sculas\n",
    "    clean_tweet = tweet.lower()\n",
    "    # removemos la URL usando expresiones regulares\n",
    "    clean_tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', clean_tweet) # ya no reemplazamos por URL, sino por vacio\n",
    "    # Quitamos los usuarios\n",
    "    clean_tweet = re.sub('@[^\\s]+', '', clean_tweet)\n",
    "    # Quitamos hashtag\n",
    "    clean_tweet = re.sub(r'#([^\\s]+)', r'\\1', clean_tweet)\n",
    "    # Eliminamos repetici√≥n de caracteres \n",
    "    clean_tweet = re.sub(r'(.)\\1+', r'\\1\\1', clean_tweet) \n",
    "    # Tokenizamos \n",
    "    clean_tweet = tknzr.tokenize(clean_tweet)\n",
    "    # Quitamos las stopwords\n",
    "    clean_tweet = [word for word in clean_tweet if word not in stopwords_] # nuestras stopwords\n",
    "    # lemming y stemming \n",
    "    clean_tweet = stem_lemm(clean_tweet)\n",
    "    # Encuentra las que tienen errores\n",
    "    misspelled = spell.unknown(clean_tweet)\n",
    "    # Corregimos el tweet. \n",
    "    clean_tweet = [\n",
    "        spell.correction(word) if word in misspelled else word for word in clean_tweet]\n",
    "    return clean_tweet\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simply', 'love', 'outfit', 'favorite', \"can't\", 'stop', 'think', 'fashion']\n"
     ]
    }
   ],
   "source": [
    "print(limpia_tweets(tweets[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todos nuestros Tweets\n",
    "tweets_limpios = [limpia_tweets(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'whats', 'wrong', 'world', 'today', 'bore', 'world']\n",
      "['simply', 'love', 'outfit', 'favorite', \"can't\", 'stop', 'think', 'fashion']\n",
      "['launch', 'cancel', 'want', 'believe', 'of', 'notcool', 'spacey']\n",
      "['hey', 'visit', 'get', 'free', 'coupon', 'products', 'freecoupon']\n",
      "['nice', 'album', 'release', \"can't\", 'stop', 'listen', 'best', 'hotly', 'music']\n"
     ]
    }
   ],
   "source": [
    "for t in tweets_limpios:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojis!!\n",
    "\n",
    "Para el tratamiento de emojis, comparto dos alternativas. Una es traducirlo (requiere instalar la librer√≠a emoji) o eliminarlos con expresiones regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is :thumbs_up:\n"
     ]
    }
   ],
   "source": [
    "import emoji #!pip install emoji\n",
    "\n",
    "# Traducimos usando la librer√≠a emoji\n",
    "print(emoji.demojize('Python is üëç'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is \n"
     ]
    }
   ],
   "source": [
    "def remove_emoji(string):\n",
    "    \"\"\"\n",
    "    Elimina los emojis de una cadena de texto. \n",
    "    :param string: Una cadena de texto \n",
    "    :return: Una cadena de texto sin los emojis\n",
    "    \n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "print(remove_emoji('Python is üëç'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
