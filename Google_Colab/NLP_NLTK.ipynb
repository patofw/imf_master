{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U pyspellchecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial de NLTK para NLP\n",
    "\n",
    "En este corto tutorial veremos algunas de las herramientas y métodos que tiene NLTK para el procesamiento de texto. También nos apoyaremos en la librería [RE](https://docs.python.org/3/library/re.html) (Expresiones Regulares) para la limpieza del texto. Empezamos creando una lista de Tweets sintéticos, ya que lo que nos interesa en este Notebook es la parte del procesamiento. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\patricio.fernandez\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patricio.fernandez\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\patricio.fernandez\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer # tokenizador\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords # elimina palabras de poco valor\n",
    "from nltk.stem.wordnet import WordNetLemmatizer # stemming lemming\n",
    "# Instalamos algunas de las APIs y herramientas de NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestras stopwords\n",
    "\n",
    "stopwords_ = set(stopwords.words('english') + list(punctuation)+ [\"rt\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"Hellooooooo!! WHat's wrong with the world today!!!! @user1 #bored #world\",\n",
    "    \" RT I simply looove this outfit www.fakeoutfitpage.com, it's my favorite. I can't stop thinking about it!! @shopingmall #fashion\",\n",
    "    \"The launch of the @SpaceY has been cancelled, I don't want to believe this :( . #notcool #spacey\",\n",
    "    \"RT Hey! Visit www.thisisafakeurl.com to get a FREEEEE coupon on ALL the @fakeshop productos. #Freecoupon\",\n",
    "    \"What a nice album @hotplay has released, I JUST CAN'T STOP LISTENING TO IT. THE BEEEEEEEEEST!! #hotplay #music\"    \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos a trabajar con un Tweet para revisar cada paso, luego creamos una función para realizarlo en todo el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" RT I simply looove this outfit www.fakeoutfitpage.com, it's my favorite. I can't stop thinking about it!! @shopingmall #fashion\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos a trabajar con un tweet para ir viendo cada paso\n",
    "tweet = tweets[1]\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" rt i simply looove this outfit www.fakeoutfitpage.com, it's my favorite. i can't stop thinking about it!! @shopingmall #fashion\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empezamos transformando a minúsculas\n",
    "tweet = tweet.lower()\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" rt i simply looove this outfit URL it's my favorite. i can't stop thinking about it!! @shopingmall #fashion\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removemos la URL usando expresiones regulares\n",
    "tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet)\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" rt i simply looove this outfit URL it's my favorite. i can't stop thinking about it!! USUARIO #fashion\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removemos el usario expresiones regulares\n",
    "tweet = re.sub('@[^\\s]+', 'USUARIO', tweet)\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" rt i simply looove this outfit URL it's my favorite. i can't stop thinking about it!! USUARIO fashion\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removemos el hashtag\n",
    "tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos la repetición de caracteres en LOOOOOOOOOOVE\n",
    "tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', 'i', 'simply', 'loove', 'this', 'outfit', 'URL', 'it', \"'s\", 'my', 'favorite', '.', 'i', 'ca', \"n't\", 'stop', 'thinking', 'about', 'it', '!', '!', 'USUARIO', 'fashion']\n"
     ]
    }
   ],
   "source": [
    "# probamos los tokenizadores que cargamos \n",
    "# word tokenizer \n",
    "print(word_tokenize(tweet)) # remove repeated characters (helloooooooo into hello)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No me gusta mucho el resultado ya que palabras como it's se dividen, igual can't. Probamos el otro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', 'i', 'simply', 'loove', 'this', 'outfit', 'URL', \"it's\", 'my', 'favorite', '.', 'i', \"can't\", 'stop', 'thinking', 'about', 'it', '!', '!', 'USUARIO', 'fashion']\n"
     ]
    }
   ],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "print(tknzr.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es mejor, y además está diseñado para Twitter :). Lo utilizaremos de ahora en adelante "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', 'i', 'simply', 'loove', 'this', 'outfit', 'URL', \"it's\", 'my', 'favorite', '.', 'i', \"can't\", 'stop', 'thinking', 'about', 'it', '!', '!', 'USUARIO', 'fashion']\n"
     ]
    }
   ],
   "source": [
    "# TOkenizamos \n",
    "tweet = tknzr.tokenize(tweet)\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simply', 'loove', 'outfit', 'URL', 'favorite', \"can't\", 'stop', 'thinking', 'USUARIO', 'fashion']\n"
     ]
    }
   ],
   "source": [
    "# Quitamos las stopwords\n",
    "tweet = [word for word in tweet if word not in stopwords_] # nuestras stopwords\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simply', 'loove', 'outfit', 'URL', 'favorite', \"can't\", 'stop', 'think', 'USUARIO', 'fashion']\n"
     ]
    }
   ],
   "source": [
    "def stem_lemm(tweet_list):\n",
    "    \"\"\"\n",
    "    Lemmatiza un tweeet uitlizando el lematizador de NLTK\n",
    "    :param tweet_list: Un tweet tokenizado previamente\n",
    "    :return: Una lista de tokens\n",
    "    \"\"\"\n",
    "    lem = WordNetLemmatizer()\n",
    "    normalized_tweet = []\n",
    "    for word in tweet_list:\n",
    "        normalized_text = lem.lemmatize(word,'v')\n",
    "        normalized_tweet.append(normalized_text)\n",
    "    return normalized_tweet\n",
    "\n",
    "print(stem_lemm(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrección gramatical \n",
    "\n",
    "En muchos casos también es posible que necesitemos añadir un corrector gramatical para nuestro proyecto. Eso se puede hacer mediante varias librerías, pero una de las más comunes es SpellChecker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "{'love', 'looe', 'loose'}\n",
      "usuario\n",
      "{'usuario'}\n",
      "usl\n",
      "{'curl', 'ural', 'ura', 'uri', 'urn', 'crl', 'usl', 'ucl', 'rl', 'ure', 'urc', 'uhl', 'urd', 'erl', 'hurl', 'purl', 'brl', 'ur', 'ual', 'urr', 'u/l'}\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Encuentra las que tienen errores\n",
    "misspelled = spell.unknown(tweet)\n",
    "\n",
    "for word in misspelled:\n",
    "    # La correción más probable\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # otros conadidatos\n",
    "    print(spell.candidates(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simply',\n",
       " 'love',\n",
       " 'outfit',\n",
       " 'URL',\n",
       " 'favorite',\n",
       " \"can't\",\n",
       " 'stop',\n",
       " 'thinking',\n",
       " 'USUARIO',\n",
       " 'fashion']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Corregimos el tweet. \n",
    "[spell.correction(word) if word in misspelled else word for word in tweet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo junto \n",
    "\n",
    "Ahora que hemos visto y probado varios métodos. Podemos hacer una función o una clase con todo lo necesario para limpiar nuestros Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpia_tweets(tweet): \n",
    "    # empezamos transformando a minúsculas\n",
    "    clean_tweet = tweet.lower()\n",
    "    # removemos la URL usando expresiones regulares\n",
    "    clean_tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', clean_tweet) # ya no reemplazamos por URL, sino por vacio\n",
    "    # Quitamos los usuarios\n",
    "    clean_tweet = re.sub('@[^\\s]+', '', clean_tweet)\n",
    "    # Quitamos hashtag\n",
    "    clean_tweet = re.sub(r'#([^\\s]+)', r'\\1', clean_tweet)\n",
    "    # Eliminamos repetición de caracteres \n",
    "    clean_tweet = re.sub(r'(.)\\1+', r'\\1\\1', clean_tweet) \n",
    "    # Tokenizamos \n",
    "    clean_tweet = tknzr.tokenize(clean_tweet)\n",
    "    # Quitamos las stopwords\n",
    "    clean_tweet = [word for word in clean_tweet if word not in stopwords_] # nuestras stopwords\n",
    "    # lemming y stemming \n",
    "    clean_tweet = stem_lemm(clean_tweet)\n",
    "    # Encuentra las que tienen errores\n",
    "    misspelled = spell.unknown(clean_tweet)\n",
    "    # Corregimos el tweet. \n",
    "    clean_tweet = [\n",
    "        spell.correction(word) if word in misspelled else word for word in clean_tweet]\n",
    "    return clean_tweet\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simply', 'love', 'outfit', 'favorite', \"can't\", 'stop', 'think', 'fashion']\n"
     ]
    }
   ],
   "source": [
    "print(limpia_tweets(tweets[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todos nuestros Tweets\n",
    "tweets_limpios = [limpia_tweets(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'whats', 'wrong', 'world', 'today', 'bore', 'world']\n",
      "['simply', 'love', 'outfit', 'favorite', \"can't\", 'stop', 'think', 'fashion']\n",
      "['launch', 'cancel', 'want', 'believe', 'of', 'notcool', 'spacey']\n",
      "['hey', 'visit', 'get', 'free', 'coupon', 'products', 'freecoupon']\n",
      "['nice', 'album', 'release', \"can't\", 'stop', 'listen', 'best', 'hotly', 'music']\n"
     ]
    }
   ],
   "source": [
    "for t in tweets_limpios:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
